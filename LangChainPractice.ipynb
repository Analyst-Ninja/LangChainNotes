{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c572b80-e399-4c0d-a4b1-b5127c69ac97",
   "metadata": {},
   "source": [
    "#### **Output Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a96c54e1-3dd1-4c2f-b9cd-76051edbf27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d1db38-4081-4f33-ae83-75ef4f84d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Ollama(model='llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbe0f2bb-b976-4d42-88b9-1f0a1f79b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name='bad_string', description='This is a poorly formatted user input string'),\n",
    "    ResponseSchema(name='good_string', description='This is your response, a reformatted response'),\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed7b3f1e-ae4c-4ca6-bb0d-76799895a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a601279-ce78-4e88-bafd-adff09d5c3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled correctly\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "%USER INPUT:\n",
      "Welcome to Californiya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You are will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "%USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['user_input'],\n",
    "    partial_variables={\"format_instructions\" : format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"Welcome to Californiya!\")\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "678d7d40-9861-48f2-bfab-db4f1d0d11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_output = chat.invoke(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebc398ff-7d60-4633-ac97-e6a777041d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the reformatted string:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"bad_string\": \"Welcome to Californiya!\",\n",
      "    \"good_string\": \"Welcome to California!\"\n",
      "}\n",
      "```\n",
      "\n",
      "Note that I corrected the spelling of \"Californya\" to \"California\". Let me know if you have any further requests!\n"
     ]
    }
   ],
   "source": [
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aecf4901-c3a0-418b-a99f-ac7be2b35488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bad_string': 'Welcome to Californiya!', 'good_string': 'Welcome to California!'}\n"
     ]
    }
   ],
   "source": [
    "parsed_output = output_parser.parse(llm_output)\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad29e576-421a-461f-a2a2-4b8dff92a0c6",
   "metadata": {},
   "source": [
    "#### **Text Splitters**\n",
    "Often times your document is too long (like a book) for your LLM. You need to split it up into chunks. Text splitters help with this.\n",
    "\n",
    "There are many ways you could split your text into chunks, experiment with [different ones](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) to see which is best for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6218a84-e165-44ce-8535-db213a3e92c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data : 1\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open('hp.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(f'Length of the data : {len([data])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e59b75a7-5fa6-4f95-a1cf-f0c9ffde2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ") \n",
    "\n",
    "texts = text_splitter.create_documents([data]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87841aff-84bd-419f-94ef-192813192c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 532 documents.\n"
     ]
    }
   ],
   "source": [
    "print(f'You have {len(texts)} documents.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a522da-c3a1-458f-bc1c-d8bb35ee6ad4",
   "metadata": {},
   "source": [
    "#### **Retrievers**\n",
    "Easy way to combine documents with language models.\n",
    "\n",
    "There are many different types of retrievers, the most widely supported is the VectoreStoreRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c8a7603-1e05-4df2-b187-eee1a0a4ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "# from langchain.embeddings import OllamaEmbeddings # Depricated \n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "loader = TextLoader('hp.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce4414e6-7402-4417-8024-63ab58fa71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OllamaEmbeddings(model='llama3')\n",
    "\n",
    "db = FAISS.from_documents(texts[:1], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6937e545-79d9-4511-899e-80284b1e1fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1531e704-a8f8-4a53-a7c5-67cea599b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents('What is the place name here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9964676d-7384-479e-a6c2-9643088ed080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Document(id='c97a7e45-5940-41f4-ab84-d9453372387c', metadata={'source': 'hp.txt'}, page_content=\"Harry Potter and the Sorcerer's Stone\\n\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\\nthat they were perfectly normal, thank you very much. They were the last\\npeople you'd expect to be involved in anything strange or mysterious,\\nbecause they just didn't hold with such nonsense.\\n\\nMr. Dursley was the director of a firm called Grunnings, which made\\ndrills. He was a big, beefy man with hardly any neck, although he did\\nhave a very large mustache. Mrs. Dursley was thin and blonde and had\\nnearly twice the usual amount of neck, which came in very useful as she\\nspent so much of her time craning over garden fences, spying on the\\nneighbors. The Dursleys had a small son called Dudley and in their\\nopinion there was no finer boy anywhere.\")],\n",
       " 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs, len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b6236443-2167-4c5a-a6d3-31e568aa29c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorcerer's Stone\n",
      "\n",
      "\n",
      "CHAPTER ONE\n",
      "\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
      "that they were perfectly normal, thank you very much. They were the last\n",
      "people you'd expect to be involved in anything strange or mysterious,\n",
      "because they just didn't hold with such nonsense.\n",
      "\n",
      "Mr. Dursley was the director of a firm called Grunnings, which made\n",
      "drills. He was a big, beefy man with hardly any neck, although he did\n",
      "have a very large mustache. Mrs. Dursley was thin and blonde and had\n",
      "nearly twice the usual amount of neck, which came in very useful as she\n",
      "spent so much of her time craning over garden fences, spying on the\n",
      "neighbors. The Dursleys had a small son called Dudley and in their\n",
      "opinion there was no finer boy anywhere.\n"
     ]
    }
   ],
   "source": [
    "l = \"\\n\".join([x.page_content for x in docs])\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c70cd-6dae-4920-81a3-762ea238ab62",
   "metadata": {},
   "source": [
    "#### **Chat Message History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8bc9693-5ab0-45b8-9879-f4610c37892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "chat = Ollama(model='llama3')\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message('Hi')\n",
    "\n",
    "history.add_user_message('What is a capital for India?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "da0bebfe-595e-45e3-bac9-138148140de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is a capital for India?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "47eaa0ce-0c9a-4ade-bb53-3a13f225b734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is New Delhi.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chat.invoke(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f85700e4-6c45-44ec-a7ed-0b69131b813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_ai_message(ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf4d871c-6444-44e2-a55e-c6a44fff2d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is a capital for India?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The capital of India is New Delhi.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac32f8c-9bd0-49d8-aa8b-3b04aa38b188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
